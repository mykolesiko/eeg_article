{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MADE_RACNN_beta1_article2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mykolesiko/egg_article/blob/main/MADE_RACNN_beta1_article2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bA0MKeOZaZ3X"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchtext\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
        "import numpy as np\n",
        "from  scipy import stats\n",
        "import scipy\n",
        "np.random.seed(1)\n",
        "torch.manual_seed(1)\n",
        "random.seed(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "4w2LA1ajrNf4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f2ea4ee-754c-4b61-83c7-556164ee9c11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdvxX7ilpEh0"
      },
      "source": [
        "import os\n",
        "os.chdir(\"/content/drive/MyDrive/MADE/Project/deap\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "writer = SummaryWriter()"
      ],
      "metadata": {
        "id": "1QTqzADBqT5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YhW3-JQpEh6"
      },
      "source": [
        "class Args:\n",
        "  def __init__(self): #(data_path, epoch, batch_siz, image_size, learning_rate, weight_deca, learning_rate, learning_rate_gamma, weight_bce, load, output_dir)\n",
        "    self.epochs = 2\n",
        "    self.batch_size = 1000\n",
        "    self.lr= 3e-4\n",
        "    self.weight_decay= 1e-6\n",
        "    self.learning_rate=None\n",
        "    self.learning_rate_gamma=None\n",
        "    self.weight_bce=1\n",
        "    self.load=None\n",
        "    self.output_dir=\"/content/drive/MyDrive/MADE/Project/RACNN_models/\"\n",
        "    self.data_dir =\"./data_preprocessed_python/\"# \"/content/drive/MyDrive/MADE/Project/train/physionet.org/\"\n",
        "args = Args()   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_skYteMedLx"
      },
      "source": [
        "Сетка по статье https://drive.google.com/file/d/1ia3W9vgdZLurEx4R7ndVL05clQLEarKF/view?usp=sharing ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quQeqXQ4b7K-"
      },
      "source": [
        "def get_padding(in_size, kernel_size, stride):\n",
        "    if (in_size % stride == 0):\n",
        "        padding = max(kernel_size - stride, 0)\n",
        "    else:\n",
        "        padding = max(kernel_size - (in_size % stride), 0)\n",
        "    return (padding)    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUqO7ou9aAH-"
      },
      "source": [
        "def get_temporal_feature_extractor(input_size, batch_norm):\n",
        "  pad = get_padding(input_size, 5,  2)\n",
        "  conv1 = nn.Conv3d(1, 32, kernel_size = (1, 1, 5), stride=(1, 1, 2), padding=(0, 0, 2))\n",
        "  bn1 = nn.BatchNorm3d(32)\n",
        "  relu1 = nn.ReLU()\n",
        "  pad = get_padding(input_size, 3,  2)\n",
        "  conv2 = nn.Conv3d(32, 32, kernel_size = (1, 1, 3), stride=(1, 1, 2), padding=(0, 0, pad))\n",
        "  bn2 = nn.BatchNorm3d(32)\n",
        "  relu2 = nn.ReLU()\n",
        "  conv3 = nn.Conv3d(32, 32, kernel_size = (1, 1, 3), stride=(1, 1, 2), padding=(0, 0, pad))\n",
        "  bn3 = nn.BatchNorm3d(32)\n",
        "  relu3 = nn.ReLU()\n",
        "  conv4 = nn.Conv3d(32, 32, kernel_size = (1, 1, 16), stride=(1, 1, 16), padding=0)\n",
        "  bn4 = nn.BatchNorm3d(32)\n",
        "  relu4 = nn.ReLU()\n",
        "  #print(\"11\")\n",
        "  if batch_norm == True:\n",
        "      return (torch.nn.Sequential(conv1, bn1, relu1, conv2, bn2, relu2, conv3, bn3, relu3, conv4, bn4 ,relu4))\n",
        "  else:\n",
        "      return (torch.nn.Sequential(conv1, relu1, conv2, relu2, conv3, relu3, conv4, relu4))\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIxL2BZYcic1"
      },
      "source": [
        "def get_regional_feature_extractor(batch_norm):\n",
        "  conv1 = nn.Conv2d(32, 32, kernel_size = (3, 3), stride=1 , padding='same')\n",
        "  bn1 =   nn.BatchNorm2d(32)\n",
        "  relu1 = nn.ReLU()\n",
        "  conv2 = nn.Conv2d(32, 32, kernel_size = (3, 3), stride=1 , padding='same')\n",
        "  bn2 =   nn.BatchNorm2d(32)\n",
        "  relu2 = nn.ReLU()\n",
        "  if batch_norm == True:\n",
        "      return(torch.nn.Sequential(conv1, bn1, relu1, conv2, bn2, relu2))\n",
        "  else:    \n",
        "      return(torch.nn.Sequential(conv1, relu1, conv2, relu2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmu1NPeMeWsK"
      },
      "source": [
        "class Asymmetric_feature_extractor(torch.nn.Module): \n",
        "   def __init__(self, batch_norm = True):\n",
        "     super().__init__()\n",
        "     self.conv = nn.Conv2d(32, 64, kernel_size = 1, stride=1 , padding='same')\n",
        "     self.relu = nn.ReLU()\n",
        "     self.bn =   nn.BatchNorm2d(64)\n",
        "     self.batch_norm = batch_norm\n",
        "   def forward(self, input):\n",
        "     #input(bs, nf, h, w)\n",
        "     #print(input.shape)\n",
        "     half_mat = torch.split(input, (4, 1, 4), dim = 3)\n",
        "     temp = half_mat[2][:, :, :, [3, 2, 1, 0]]\n",
        "     input_new = half_mat[0] - temp\n",
        "     #print(input_new)\n",
        "     #print(half_mat[0].shape)\n",
        "     #print(half_mat[2].shape)\n",
        "     #input_new =  half_mat[0] -  half_mat[2]\n",
        "     #print(input.shape)\n",
        "     output = self.conv(input_new)\n",
        "     if self.batch_norm:\n",
        "          output = self.bn(output)\n",
        "     output = self.relu(output)\n",
        "     #print(output.shape)\n",
        "     return output\n",
        "\n",
        "def get_asymmetric_feature_extractor(batch_norm  = True):\n",
        "    return (Asymmetric_feature_extractor())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TM4dD2hIeK4U"
      },
      "source": [
        "LEN_RECORD_IN_SECONDS = 60\n",
        "NVIDEOS = 40\n",
        "HCANALS = 9\n",
        "WCANALS = 9\n",
        "NTIMES_IN_SAMPLE = 128\n",
        "NTIMES_IN_SEC = 128\n",
        "NCANALS = 32\n",
        "NFEATURES = 32\n",
        "NSUBJECTS = 32\n",
        "electrode_matrix = {}\n",
        "electrode_matrix['FP1'] = [0, 3]\n",
        "electrode_matrix['FP2'] = [0, 5]\n",
        "electrode_matrix['AF3'] = [1, 3]\n",
        "electrode_matrix['AF4'] = [1, 5]\n",
        "electrode_matrix['F7']  = [2, 0]\n",
        "electrode_matrix['F3']  = [2, 2]\n",
        "electrode_matrix['FZ']  = [2, 4]\n",
        "electrode_matrix['F4']  = [2, 6]\n",
        "electrode_matrix['F8']  = [2, 8]\n",
        "electrode_matrix['FC5']  = [3, 1]\n",
        "electrode_matrix['FC1']  = [3, 3]\n",
        "electrode_matrix['FC2']  = [3, 5]\n",
        "electrode_matrix['FC6']  = [3, 7]\n",
        "electrode_matrix['T7']  = [4, 0]\n",
        "electrode_matrix['C3']  = [4, 2]\n",
        "electrode_matrix['CZ']  = [4, 4]\n",
        "electrode_matrix['C4']  = [4, 6]\n",
        "electrode_matrix['T8']  = [4, 8]\n",
        "electrode_matrix['CP5']  = [5, 1]\n",
        "electrode_matrix['CP1']  = [5, 3]\n",
        "electrode_matrix['CP2']  = [5, 5]\n",
        "electrode_matrix['CP6']  = [5, 7]\n",
        "electrode_matrix['P7']  = [6, 0]\n",
        "electrode_matrix['P3']  = [6, 2]\n",
        "electrode_matrix['PZ']  = [6, 4]\n",
        "electrode_matrix['P4']  = [6, 6]\n",
        "electrode_matrix['P8']  = [6, 8]\n",
        "electrode_matrix['PO3'] = [7, 3]\n",
        "electrode_matrix['PO4'] = [7, 5]\n",
        "electrode_matrix['O1'] = [8, 3]\n",
        "electrode_matrix['OZ'] = [8, 4]\n",
        "electrode_matrix['O2'] = [8, 5]\n",
        "\n",
        "list_electrodes = ['FP1', 'AF3', 'F3', 'F7', 'FC5', 'FC1', 'C3',\t'T7',\t'CP5',\t'CP1',\t'P3',\t'P7',\t'PO3',\t'O1',\t'OZ',\t'PZ',\t'FP2',\t'AF4', 'FZ', 'F4', 'F8', 'FC6',\t'FC2',\t'CZ', 'C4', 'T8', 'CP6',\t'CP2',\t'P4', \t'P8',\t'PO4',\t'O2']\n",
        "data_dir = './data_preprocessed_python'\n",
        "TRAIN_SIZE = 0.9\n",
        "THRESHOLD = 4.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OngEyl2JXu0"
      },
      "source": [
        "from collections import Counter\n",
        "import scipy\n",
        "from scipy.fft import fft, ifft\n",
        "\n",
        "\n",
        "class RandomAugmentation(object):\n",
        "    def __init__(self, augmenters, probability):\n",
        "        self._augmenters = augmenters\n",
        "        self.probability = probability\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        augmenter = random.choice(self._augmenters)\n",
        "        #print(augmenter)\n",
        "        if (augmenter):\n",
        "            if (np.random.random() < self.probability):\n",
        "               return augmenter(sample)\n",
        "        return(sample)  \n",
        "\n",
        "\n",
        "class add_noise(object):\n",
        "    def __init__(self, ):\n",
        "        super(add_noise, self).__init__()\n",
        "\n",
        "    def __call__(self, sample: dict):\n",
        "        white_noise = np.random.normal(loc=0.0, scale=1.0, size=sample['data'].shape)\n",
        "        sample['data'] = sample['data'] + white_noise\n",
        "        return sample\n",
        "\n",
        "\n",
        "class reset_part_in_time(object):\n",
        "    def __init__(self, percentage):\n",
        "        super(reset_part_in_time, self).__init__()\n",
        "        self.percentage = percentage\n",
        "\n",
        "    def __call__(self, sample: dict):\n",
        "        len_data = len(sample['data'][0])\n",
        "        interval_to_reset = int(len_data * self.percentage)\n",
        "        for ncanal in range(NVIDEOS):\n",
        "            begin = random.randint(0, len_data - interval_to_reset)\n",
        "            sample['data'][ncanal, begin : begin + interval_to_reset] = 0\n",
        "\n",
        "        return sample\n",
        "\n",
        "\n",
        "class reset_part_in_freq(object):\n",
        "    def __init__(self, percentage):\n",
        "        super(reset_part_in_freq, self).__init__()\n",
        "        self.percentage = percentage\n",
        "\n",
        "    def __call__(self, sample: dict):\n",
        "        len_data = len(sample['data'][0])\n",
        "        interval_to_reset = int(len_data * self.percentage)\n",
        "        power_freq_data = fft(sample['data'])\n",
        "        for ncanal in range(NVIDEOS):\n",
        "            begin = random.randint(0, len_data - interval_to_reset)\n",
        "            power_freq_data[ncanal, begin : begin + interval_to_reset] = 0\n",
        "        sample['data'] = ifft(power_freq_data)\n",
        "        return sample\n",
        "\n",
        "       \n",
        "\n",
        "class to_head_matrix(object):\n",
        "    def __init__(self):\n",
        "        super(to_head_matrix, self).__init__()\n",
        "        \n",
        "\n",
        "    def __call__(self, sample: dict):\n",
        "        input = sample['data'].copy()\n",
        "        sample['data'] = np.zeros((HCANALS, WCANALS, NTIMES_IN_SAMPLE))\n",
        "        #sample_from_one_canals = []\n",
        "        #for i_canal in range(NCANALS):\n",
        "        #     sample_from_one_canal = input[i_canal]\n",
        "        #     sample_from_one_canals.append(sample_from_one_canal)\n",
        "        #print(sample_from_one_canal.shape)\n",
        "        #sample_from_one_canals = np.asarray(sample_from_one_canals).copy()\n",
        "      # здесь делаем нормализацию фактически по поверхности головы (по одному времени t для всех каналов)\n",
        "        #sample_from_one_canals = scipy.stats.zscore(input, axis = 0)\n",
        "        sample_from_one_canals = input\n",
        "        for i_canal in range(NCANALS):\n",
        "             sample['data'][electrode_matrix[list_electrodes[i_canal]][0],  electrode_matrix[list_electrodes[i_canal]][1]] = sample_from_one_canals[i_canal]\n",
        "        return sample     \n",
        "\n",
        "class ToTensor(object):\n",
        "\n",
        "    def __init__(self, ):\n",
        "        super(ToTensor, self).__init__()\n",
        "\n",
        "    def __call__(self, sample: dict):\n",
        "        return {\"labels\": torch.tensor(sample[\"labels\"], dtype=torch.long),\n",
        "                \"data\": torch.tensor(sample[\"data\"], dtype=torch.float32),\n",
        "                } \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class EmotionDataset(Dataset):\n",
        "    def __init__ (self, data, labels_bin, transforms, indexes, interval = 1):  #indexes - индексы видео которые вошли в датасет, data - данные labels - метки бинарные\n",
        "       self.data_samples = []\n",
        "       self.labels = []\n",
        "       self.transforms = transforms\n",
        "       self.cnt = [Counter(), Counter(), Counter(),Counter()]\n",
        "       for sub in range(len(data)): #sub   - человек\n",
        "          for nvideo in range(NVIDEOS):\n",
        "             \n",
        "             for nsec in range(LEN_RECORD_IN_SECONDS):\n",
        "                    self.data_samples.append(data[sub][nvideo, :,  (3 + nsec) * NTIMES_IN_SEC : (3 + nsec + interval) * NTIMES_IN_SEC])\n",
        "                    self.labels.append(labels_bin[sub][nvideo, :])\n",
        "                    \n",
        "\n",
        "       self.data_samples = np.array(self.data_samples)[np.array(indexes)]\n",
        "       self.labels = np.array(self.labels)[np.array(indexes)]\n",
        "       for i in range(4):\n",
        "          self.cnt[i].update(self.labels[:, i])\n",
        "       \n",
        "\n",
        "    def __len__(self):\n",
        "       result =  len(self.data_samples)\n",
        "       return result\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "       sample = {}   \n",
        "       sample = {\"labels\": self.labels[item],\n",
        "                 \"data\": self.data_samples[item]\n",
        "       }\n",
        "\n",
        "       if self.transforms is not None:\n",
        "           for t in self.transforms:\n",
        "                sample = t(sample)\n",
        "       #print(sample)         \n",
        "       return sample\n",
        "                 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9H17l1JJYOo"
      },
      "source": [
        "class EmotionNet(torch.nn.Module): \n",
        "   def __init__(self, hcanals, wcanals, nfeatures, ntimes_in_sample, batch_norm = True, reg_extractor = True, ass_extractor = True):\n",
        "      super().__init__()\n",
        "      #print(\"1\")\n",
        "      self.reg_extractor = reg_extractor\n",
        "      self.ass_extractor = ass_extractor\n",
        "      self.tfe = get_temporal_feature_extractor(ntimes_in_sample, batch_norm ) #(bs, 1, h = 9, w = 9, s = 128) -> (bs, h = 9, w = 9, s = 1)\n",
        "      #print(\"2\")\n",
        "      if (reg_extractor):\n",
        "          self.rfe = get_regional_feature_extractor(batch_norm) #(h = 9, w = 9, s = 32) -> (h = 9, w = 9, s = 32)\n",
        "      #print(\"3\")\n",
        "      if ass_extractor:\n",
        "          self.afe = get_asymmetric_feature_extractor(batch_norm)\n",
        "      #print(\"4\")\n",
        "      self.flat1 = nn.Flatten(1, 3)\n",
        "      self.flat2 = nn.Flatten(1, 3)\n",
        "      self.input_linear_size = 0\n",
        "      if (not reg_extractor and not ass_extractor) :\n",
        "          self.input_linear_size =  hcanals * wcanals * nfeatures\n",
        "      else:\n",
        "          if (ass_extractor) :   \n",
        "              self.input_linear_size += int(hcanals * (wcanals//2)* nfeatures * 2)\n",
        "          if (reg_extractor) :   \n",
        "              self.input_linear_size += hcanals * (wcanals)* nfeatures    \n",
        "      #print(self.input_linear_size)\n",
        "      #print(\"5\")\n",
        "      self.fc1 = nn.Linear(self.input_linear_size, 20)\n",
        "      self.relu1 = nn.ReLU()\n",
        "      self.drop = nn.Dropout(0.3)\n",
        "      self.fc2 = nn.Linear(20, 2)\n",
        "   def forward(self, input):\n",
        "      input = input.unsqueeze(1)\n",
        "      #print(f\"input_shape = {input.shape}\")\n",
        "      \n",
        "      #input (bs, in_canals = 1,  h=9, w=9, s=128)\n",
        "      output_tfe = self.tfe(input)\n",
        "      #print(f\"output_tfe.shape = {output_tfe.shape}\")\n",
        "      #output_tfe (bs, in_canals = 32,  h=9, w=9, s=1)\n",
        "      output_tfe = output_tfe.squeeze(4)\n",
        "      #print(f\"output_tfe.shape = {output_tfe.shape}\")\n",
        "      \n",
        "      #output_rfe (bs, canals = 32,  h=9, w=9)\n",
        "\n",
        "      if (self.reg_extractor) :   \n",
        "            output_rfe = self.rfe(output_tfe)\n",
        "\n",
        "      #print(f\"output_rfe.shape = {output_rfe.shape}\")\n",
        "      if (self.ass_extractor) :   \n",
        "            output_afe = self.afe(output_tfe)\n",
        "\n",
        "      #output_rfe = output_rfe.permute(0, 2, 3, 1) \n",
        "      #output_rfe (bs, h=9, w=9, 32)\n",
        "      #output_afe = output_afe.permute(0, 2, 3, 1) \n",
        "      #output_afe (bs, h=9, w=4, 64)\n",
        "      #print(f\"output_afe.shape = {output_afe.shape}\")\n",
        "      #output_rfe (bs, canals = 64,  h=4, w=9)\n",
        "      if (self.reg_extractor) :   \n",
        "          output_rfe_flatten = self.flat1(output_rfe)\n",
        "      #print(f\"output_rfe_flatten.shape = {output_rfe_flatten.shape}\")\n",
        "      if (self.ass_extractor) :   \n",
        "          output_afe_flatten = self.flat2(output_afe)\n",
        "      if (not self.reg_extractor and not self.ass_extractor) :          \n",
        "          output_tfe_flatten = self.flat1(output_tfe)\n",
        "          output1 = self.drop(self.fc1(output_tfe_flatten))\n",
        "      if (self.reg_extractor) and (self.ass_extractor):   \n",
        "          output1 = self.drop(self.fc1(torch.cat((output_rfe_flatten, output_afe_flatten), dim = 1)))\n",
        "      else:    \n",
        "          if (self.reg_extractor) :   \n",
        "               output1 = self.drop(self.fc1(output_rfe_flatten))        \n",
        "          if (self.ass_extractor) :   \n",
        "               output1 = self.drop(self.fc1(output_afe_flatten))             \n",
        "\n",
        "      #print(f\"output_afe_flatten.shape = {output_afe_flatten.shape}\")\n",
        "      #output1 = self.drop(self.fc1(torch.cat((output_rfe_flatten, output_afe_flatten), dim = 1)))\n",
        "      #print(f\"output1.shape = {output1.shape}\")\n",
        "      output1_relu = self.relu1(output1)\n",
        "      #print(f\"output1_relu.shape = {output1_relu.shape}\")\n",
        "      output2 = self.fc2(output1_relu)\n",
        "      #print(f\"output2.shape = {output2.shape}\")\n",
        "      return output2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53e0zIUWljhm"
      },
      "source": [
        "def get_model(batch_norm = False, reg_extractor = True, ass_extractor = True):\n",
        "  model = EmotionNet(HCANALS, WCANALS, NFEATURES, NTIMES_IN_SAMPLE, batch_norm, reg_extractor, ass_extractor).to(device)\n",
        "  return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E72YYXmOvfE4"
      },
      "source": [
        "Считываем данные"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXFYsRzpvPpS"
      },
      "source": [
        "import glob\n",
        "import pickle\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "data = []\n",
        "labels = []\n",
        "data_dir = './data_preprocessed_python'\n",
        "files = glob.glob(os.path.join(data_dir, \"*.dat\"))\n",
        "data_raw = []\n",
        "for file_data in files:\n",
        "    raw_data = pickle.load(open(file_data, 'rb'), encoding='latin1')\n",
        "    data.append(raw_data['data'])\n",
        "    labels.append(raw_data['labels'])\n",
        "    #break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(files)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09K2kh0A1mJV",
        "outputId": "38210ed7-87fb-4420-9220-01e925c81ed0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P67CS5aW2fX6"
      },
      "source": [
        "# def remove_baseline():\n",
        "#   for sub in range(NSUBJECTS):\n",
        "#     for nvideo in range(NVIDEOS):\n",
        "#         for ncanal in range(NCANALS):\n",
        "#             baseline_mean = (data[sub][nvideo, ncanal, 0: 128] + data[sub][nvideo, ncanal, 128: 128  *2] + data[sub][nvideo, ncanal, 128 * 2: 128 * 3])/3\n",
        "#             #print(baseline_mean.shape)\n",
        "#             for nsec in range(60):\n",
        "#                      data[sub][nvideo, ncanal, nsec * 128 : nsec * 128 + 128] = data[sub][nvideo, ncanal, nsec * 128 : nsec * 128 + 128] - baseline_mean\n",
        "\n",
        "# remove_baseline()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbbJ-Dp5zC-h"
      },
      "source": [
        "def normalize_data():\n",
        "  for sub in range(NSUBJECTS):\n",
        "    for nvideo in range(NVIDEOS):\n",
        "        for ncanal in range(NCANALS):\n",
        "             #std = np.std(data[sub][nvideo, ncanal, :])\n",
        "             #data[sub][nvideo, ncanal, :] = np.clip(data[sub][nvideo, ncanal, :], -3 * std, 3 * std)\n",
        "             scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "             data[sub][nvideo, ncanal, :] = scaler.fit_transform(data[sub][nvideo, ncanal, :].reshape(-1, 1)).reshape(1, -1)\n",
        "             #data[sub][nvideo, ncanal, :]  = data[sub][nvideo, ncanal, :]  - data[sub][nvideo, ncanal, :].mean()\n",
        "\n",
        "normalize_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2I3BAktX5VI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdKht1Hrvi8K"
      },
      "source": [
        "Бинаризуем метки и разбиваем видео для каждого человека в соответствии с метками этого человека - оно не всегда будет сбалансированным, как оказалось, но хотя бы метки буду представлены наиболее равномерно. Выведем количестов меток 1 для каждого человека в разбиении"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtFNLIZjtn2c"
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold \n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "k  = 5\n",
        "labels_bin = []\n",
        "for sub in range(32):\n",
        "  temp = labels[sub] >= 4.5\n",
        "  #print(labels[i])\n",
        "  #print(temp)\n",
        "  labels_bin.append(temp)\n",
        "  #print(sum(labels_bin[sub][:, type_emotion]), end=' ')\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dq4P31_Z6y4i"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22fY2zTYCQpH"
      },
      "source": [
        "распределение положительных меток в датасете по каждой эмоции"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5In2kzycPvri"
      },
      "source": [
        "# print(train_dataset.cnt)\n",
        "# print(val_dataset.cnt)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRGvG4QaaYl6"
      },
      "source": [
        "def train(model, loader, criterion, optimizer, device, val_dataloader, val_f1_min,  description, type_emotion, batch = None, writer = None):\n",
        "    model.train()\n",
        "    train_loss = []\n",
        "    inputs = []\n",
        "    torch.autograd.set_detect_anomaly(True)\n",
        "   \n",
        "    #lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)#, mode='min', factor=0.1, patience=10, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08, verbose=False)\n",
        "    for i , batch in enumerate(loader):#, total=len(loader), desc=\"training...\", position=0 , leave = True)):\n",
        "            model.train()\n",
        "            optimizer.zero_grad()\n",
        "            src  = batch['data'].to(device)\n",
        "            trg = batch['labels'][:, type_emotion]\n",
        "            levels_pred = model(src)  # B x (2 * NUM_PTS)\n",
        "            levels_pred = levels_pred.cpu()\n",
        "            loss = criterion(levels_pred, trg) \n",
        "            train_loss.append(loss.item())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            if writer:\n",
        "                writer.add_scalar(f'{description}/training loss per batch',\n",
        "                                  loss.item(),\n",
        "                                  i)\n",
        "\n",
        "            # if (i % 100 == 0):\n",
        "            #     acc, f1 = (calculate_predictions(model, val_dataloader))\n",
        "            #     if (f1 > val_f1_min):\n",
        "            #           val_f1_min      = f1\n",
        "            #           torch.save({'model_state_dict': model.state_dict(),    'optimizer_state_dict': optimizer.state_dict(),}, os.path.join(args.output_dir, f\"val.tgz\"))\n",
        "            #break\n",
        "    return np.mean(train_loss), val_f1_min\n",
        "\n",
        "def evaluate(model, loader, criterion, device, writer, description, type_emotion):\n",
        "    \n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    history = []\n",
        "  \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for s, batch in enumerate(loader):#, total=len(loader), desc=\"validating...\", position=0 , leave = True)):\n",
        "            src  = batch['data'].to(device)\n",
        "            #print(src.shape)\n",
        "            trg = batch['labels'][:, type_emotion]\n",
        "\n",
        "\n",
        "\n",
        "            levels_pred = model(src)  # B x (2 * NUM_PTS)\n",
        "            #print(levels_pred.shape)\n",
        "            levels_pred = levels_pred.cpu()\n",
        "            loss = criterion(levels_pred, trg) \n",
        "            epoch_loss += loss.item() \n",
        "\n",
        "\n",
        "            if writer:\n",
        "                writer.add_scalar(f'{description}/val loss per batch',\n",
        "                                  loss.item(),\n",
        "                                  s)\n",
        "        \n",
        "    return epoch_loss / s\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix,classification_report\n",
        "\n",
        "def calculate_predictions(model, loader, type_emotion, show):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    history = []\n",
        "    real = []\n",
        "    pred = []\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for i, batch in enumerate(loader):#, total=len(loader), desc=\"predicting...\", position=0 , leave = True)):\n",
        "            src  = batch['data'].to(device)\n",
        "            #print(src.shape)\n",
        "            trg = batch['labels'][:, type_emotion]\n",
        "         \n",
        "\n",
        "            levels_pred = model(src)  # B x (2 * NUM_PTS)\n",
        "            levels_pred = levels_pred.cpu()\n",
        "            #print(levels_pred.shape)\n",
        "            trg_pred = levels_pred.argmax(1)\n",
        "            \n",
        "            real.extend(trg)\n",
        "            pred.extend(trg_pred) \n",
        "\n",
        "        if show:    \n",
        "            print(accuracy_score(real, pred)) \n",
        "            print(confusion_matrix(real, pred))  \n",
        "            print(classification_report(real, pred))  \n",
        "        f1 = ((f1_score(real, pred, average = 'binary', pos_label = 0))  + (f1_score(real, pred, average = 'binary', pos_label = 1)))/2\n",
        "        return (accuracy_score(real, pred)) , f1\n",
        "        #plt.hist(real)\n",
        "def train_loop(description, type_emotion, n_epochs = 10):\n",
        "    #args.epochs = 10\n",
        "    #criterion =  fnn.mse_loss\n",
        "    train_loss_min = 10000\n",
        "    val_f1_min = -10000\n",
        "\n",
        "    #batch = next(iter(train_dataloader))\n",
        "    for epoch in range(n_epochs):\n",
        "          #logger.info(f\"Starting epoch {epoch + 1}/{args.epochs}.\")\n",
        "    \n",
        "          train_loss, val_f1_min  = train(model, train_dataloader, criterion, optimizer ,device, val_dataloader, val_f1_min ,  description , type_emotion, None,  writer)\n",
        "          #if epoch % 500 == 0:\n",
        "          if writer:\n",
        "                writer.add_scalar(f\"{description}/training loss per epoch\",\n",
        "                                        train_loss,\n",
        "                                        epoch)\n",
        "          #print(train_loss)\n",
        "\n",
        "          if (train_loss < train_loss_min):\n",
        "                 train_loss_min      = train_loss\n",
        "                 torch.save({\n",
        "                         'model_state_dict': model.state_dict(),\n",
        "                         'optimizer_state_dict': optimizer.state_dict(),\n",
        "                       },\n",
        "                       os.path.join(args.output_dir, \"train.tgz\")\n",
        "            )  \n",
        "\n",
        "          #val_loss = evaluate(model, val_dataloader, criterion, device,  writer, description )\n",
        "          # #break\n",
        "          #print(val_loss)\n",
        "          #if writer:\n",
        "          #      writer.add_scalar(f\"{description}/val loss per epoch\",\n",
        "          #                        val_loss,\n",
        "          #                        epoch)\n",
        "          acc, f1 = (calculate_predictions(model, val_dataloader, type_emotion, False))\n",
        "          #print(acc, f1)\n",
        "          if (acc > val_f1_min):\n",
        "                       val_f1_min      = acc\n",
        "                       torch.save({'model_state_dict': model.state_dict(),    'optimizer_state_dict': optimizer.state_dict(),}, os.path.join(args.output_dir, f\"val_{description}.tgz\")) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_XINdE4SBZJL"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfHGjieBBnvq"
      },
      "source": [
        "def initialize_weights(m):\n",
        "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
        "        nn.init.xavier_uniform_(m.weight.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRzQ0EM3SMyJ"
      },
      "source": [
        "#Результаты на датасете с ликом"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KDJbNYnUtzz"
      },
      "source": [
        "##Valence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6ihl23eaSgR"
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold , KFold \n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "#def get_mertics_per_subject():\n",
        "k  = 5\n",
        "type_emotion = 0\n",
        "\n",
        "acc_all = []\n",
        "f1_all = []\n",
        "for sub in range(0,32):\n",
        "    print(f\"*******{sub}*********\")\n",
        "\n",
        "    args.batch_size = 100\n",
        "    indexes = np.arange(NVIDEOS * 1 * LEN_RECORD_IN_SECONDS)\n",
        "    n = len(indexes)\n",
        "    #X = np.arange(40)\n",
        "    y = []\n",
        "    for nvideo in range(NVIDEOS):\n",
        "        y.extend(60 * [labels_bin[sub][nvideo, type_emotion]])\n",
        "    \n",
        "\n",
        "    #skf = StratifiedKFold(n_splits=k, random_state=None, shuffle=True)\n",
        "    #balanced_split = skf.split(indexes, y)\n",
        "    kf = KFold(n_splits=k, random_state=None, shuffle=True)\n",
        "    split = kf.split(indexes)\n",
        "    acc_sub = []\n",
        "    f1_sub = []\n",
        "    for fold,  (inds_train, inds_test) in  enumerate(split):\n",
        "    #for fold,  (inds_train, inds_test) in  enumerate(balanced_split):\n",
        "        print(f\"fold = {fold}\")\n",
        "        #print(inds_train, inds_test)\n",
        "        #print(sum(labels_bin[sub][inds_train, type_emotion]))\n",
        "        #print(sum(labels_bin[sub][inds_test, type_emotion]))\n",
        "        args.batch_size = 100\n",
        "        #transforms_random = RandomAugmentation([add_noise, reset_part_in_freq, reset_part_in_time, None], 0.2)\n",
        "        #transforms = [RandomAugmentation([add_noise(), reset_part_in_freq(0.2), reset_part_in_time(0.2), None], 0.2), to_head_matrix(),ToTensor()]   \n",
        "        transforms = [to_head_matrix(),ToTensor()] \n",
        "        #transforms = [RandomAugmentation([add_noise(), None], 0.5), to_head_matrix(),ToTensor()]   \n",
        "        #transforms = [RandomAugmentation([reset_part_in_time(0.4), None], 0.1), to_head_matrix(),ToTensor()]   \n",
        "\n",
        "        train_dataset = EmotionDataset(data[sub : sub + 1], labels_bin[sub : sub + 1], transforms, inds_train)\n",
        "        train_dataloader = DataLoader(train_dataset, batch_size=args.batch_size, num_workers=1,\n",
        "                                    pin_memory=True, shuffle=True, drop_last=True)\n",
        "\n",
        "        val_dataset = EmotionDataset(data[sub: sub + 1], labels_bin[sub  : sub + 1], transforms,  inds_test)\n",
        "        val_dataloader = DataLoader(val_dataset, batch_size=args.batch_size, num_workers=1,\n",
        "                              pin_memory=True, shuffle=False, drop_last=False)\n",
        "        device  = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        model = get_model(batch_norm = True, reg_extractor = True, ass_extractor = True)\n",
        "        model.apply(initialize_weights)\n",
        "        criterion = nn.CrossEntropyLoss(reduction = 'mean')#torch.nn.MSELoss()\n",
        "        #optimizer = optim.SGD(model.parameters(), lr=3e-5, momentum = 0.9)#, weight_decay=args.weight_decay)\n",
        "        optimizer = optim.Adam(model.parameters(), lr=3e-4)#, momentum = 0.9)#, weight_decay=args.weight_decay)\n",
        "        train_loop(f'val_racnn_leak_{sub}_{fold}', 0, 50)\n",
        "        description = f'val_racnn_leak_{sub}_{fold}'\n",
        "        model_state  = torch.load(os.path.join(args.output_dir, f\"val_{description}.tgz\"))\n",
        "        #   #model = Seq2Seq(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device)\n",
        "        model.load_state_dict(model_state['model_state_dict'])\n",
        "        acc, f1 = calculate_predictions(model, val_dataloader, type_emotion, False)\n",
        "        print(f\"f1 = {f1} acc = {acc}\")\n",
        "        acc_sub.append(acc)\n",
        "        f1_sub.append(f1)\n",
        "        print(acc, f1)\n",
        "    acc_all.append(acc_sub)     \n",
        "    f1_all.append(f1_sub)     \n",
        "    pd.DataFrame(f1_all).to_csv(\"f1_val_racnn_leak_result3.csv\")\n",
        "    pd.DataFrame(acc_all).to_csv(\"acc_val_racnn_leak_result3.csv\")\n",
        "\n",
        "\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-_lRMs_U37_"
      },
      "source": [
        "import pandas as pd\n",
        "pd.DataFrame(f1_all).to_csv(\"f1_val_racnn_leak_result3.csv\")\n",
        "pd.DataFrame(acc_all).to_csv(\"acc_val_racnn_leak_result3.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "414oSSCckJgD",
        "outputId": "96777d7d-1f35-4fc6-8f0c-9f352be01b8e"
      },
      "source": [
        "print(f1_all)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.9104603919177765, 0.9322993207183188, 0.8672704772007125, 0.8097204808614205, 0.8898354876615746], [0.9936020189184747, 0.9914748508098892, 0.9936101952884064, 0.9829275666447334, 0.9850904556729485], [0.918152842571015, 0.9079738562091504, 0.8394990424658784, 0.8786928104575165, 0.9307931264991547], [0.9540227567188784, 0.9468588337124338, 0.9712178449361396, 0.8739495798319328, 0.9880691393375389], [0.9901179667716633, 0.996732003894362, 1.0, 0.996732003894362, 0.969618330274068], [0.9580419580419581, 0.7991071428571428, 0.9537037037037037, 0.9390945565759939, 0.9493100535060547], [1.0, 1.0, 0.9973814992008116, 1.0, 1.0], [0.9594897639314219, 0.9972299009112472, 0.9577402787967718, 0.8914553383944435, 0.9247906498993146], [0.9617560353756673, 0.8666659678509845, 0.9392832386320513, 0.9127272727272726, 0.9587126022697949], [0.8345588235294118, 0.8256679186450286, 0.9256344312583275, 0.8288363247355284, 0.7996589940323955], [0.9812492675495137, 0.9770832338682025, 0.983333043976458, 0.9833321758455449, 0.9812499186194383], [0.9636478654258234, 1.0, 0.9744245524296675, 0.9893226559893227, 0.9957374254049446], [0.995787626151821, 0.9894547675435321, 0.9978947276084544, 0.9936786377464344, 0.995787626151821], [1.0, 0.9540337511438362, 0.992514619883041, 0.9975148976707343, 0.9975148976707343], [0.9977752955844252, 0.9955653282580978, 0.9911306565161957, 1.0, 0.9821822973700327], [0.882063882063882, 0.9801642354080204, 0.9972144685147895, 0.9597354040839804, 0.9972299009112472], [0.9518169042360971, 0.9366671064784273, 0.9497704637587526, 0.9826686645844992, 0.9345442644411717], [1.0, 0.9978700650961355, 0.98719567498355, 0.9957374254049446, 0.9850713774764408], [0.9540757749712974, 0.9964093088667629, 1.0, 0.9738803939707243, 1.0], [0.9583304396138621, 0.9895829264163966, 0.9854160969829551, 0.9729165491169666, 0.9749995659646868], [0.7959831954579416, 0.8425019889517662, 0.8593350383631714, 0.8384425865341896, 0.8317351425947044], [0.9900396339565478, 0.9975148976707343, 0.9327064101458029, 0.9875243664717348, 0.9925446930122029], [0.9263295219092933, 0.9885687612823945, 0.9838615939557827, 0.9931412567694368, 0.9907911902386617], [0.9944132777764845, 1.0, 1.0, 1.0, 1.0], [0.9890222984562607, 0.9927164577706291, 1.0, 0.9927164577706291, 0.9963754709999925], [0.6401091901728846, 0.8077983275204708, 0.8391923275661374, 0.8944444444444444, 0.8097136643711489], [0.9572324802540895, 0.9696196122281061, 0.9975148976707343, 0.943317230273752, 0.9875744883536715], [0.9931031509978878, 1.0, 0.9838615939557827, 0.997713752256479, 0.9907911902386617], [1.0, 0.9954085439344953, 0.9931031509978878, 0.9954085439344953, 0.9954212454212454], [0.9620942904524994, 0.997780233907852, 0.9730861244019138, 0.9821822973700327, 0.9955555555555555], [0.9510722880353131, 0.9808305858652195, 0.9231740499012999, 0.9766816381149224, 0.9680509764420324], [1.0, 1.0, 1.0, 0.9979166576243821, 0.9958332609941145]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wy8u74OFss1K"
      },
      "source": [
        "f1_subs = (np.array(f1_all).mean(axis = 1))\n",
        "acc_subs = (np.array(acc_all).mean(axis = 1))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "Ff7jaU0esnjc",
        "outputId": "7453b1c0-af2f-475e-abca-fc48c8fa8f4c"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "fig, ax = plt.subplots(1, 1, figsize = (7, 2))\n",
        "ax.plot(np.arange(1,33), acc_subs)\n",
        "ax.set_title(\"Valence, accuracy per subject\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Valence, accuracy per subject')"
            ]
          },
          "metadata": {},
          "execution_count": 67
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 504x144 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.mean(acc_subs))\n",
        "print(np.mean(f1_subs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxkol7u_00Vs",
        "outputId": "d5257884-99eb-41f6-b959-b097c632eb43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9620703125000001\n",
            "0.9572812127077701\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9duItQCpU6UG"
      },
      "source": [
        "##Arousal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KdhnmLSsU4Od",
        "outputId": "5f7eda27-ef24-42b9-c4de-26c7e7d7a311"
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold \n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "#def get_mertics_per_subject():\n",
        "k  = 5\n",
        "type_emotion = 1\n",
        "\n",
        "acc_all = []\n",
        "f1_all = []\n",
        "for sub in range(18,32):\n",
        "    print(f\"*******{sub}*********\")\n",
        "\n",
        "    args.batch_size = 100\n",
        "    indexes = np.arange(NVIDEOS * 1 * LEN_RECORD_IN_SECONDS)\n",
        "    n = len(indexes)\n",
        "    #X = np.arange(40)\n",
        "    y = []\n",
        "    for nvideo in range(NVIDEOS):\n",
        "        y.extend(60 * [labels_bin[sub][nvideo, type_emotion]])\n",
        "    \n",
        "\n",
        "    skf = StratifiedKFold(n_splits=k, random_state=None, shuffle=True)\n",
        "    balanced_split = skf.split(indexes, y)\n",
        "    acc_sub = []\n",
        "    f1_sub = []\n",
        "    for fold,  (inds_train, inds_test) in  enumerate(balanced_split):\n",
        "        print(f\"fold = {fold}\")\n",
        "        #print(inds_train, inds_test)\n",
        "        #print(sum(labels_bin[sub][inds_train, type_emotion]))\n",
        "        #print(sum(labels_bin[sub][inds_test, type_emotion]))\n",
        "        args.batch_size = 100\n",
        "        #transforms_random = RandomAugmentation([add_noise, reset_part_in_freq, reset_part_in_time, None], 0.2)\n",
        "        #transforms = [RandomAugmentation([add_noise(), reset_part_in_freq(0.2), reset_part_in_time(0.2), None], 0.2), to_head_matrix(),ToTensor()]   \n",
        "        transforms = [to_head_matrix(),ToTensor()] \n",
        "        #transforms = [RandomAugmentation([add_noise(), None], 0.5), to_head_matrix(),ToTensor()]   \n",
        "        #transforms = [RandomAugmentation([reset_part_in_time(0.4), None], 0.1), to_head_matrix(),ToTensor()]   \n",
        "\n",
        "        train_dataset = EmotionDataset(data[sub : sub + 1], labels_bin[sub : sub + 1], transforms, inds_train)\n",
        "        train_dataloader = DataLoader(train_dataset, batch_size=args.batch_size, num_workers=1,\n",
        "                                    pin_memory=True, shuffle=True, drop_last=True)\n",
        "\n",
        "        val_dataset = EmotionDataset(data[sub: sub + 1], labels_bin[sub  : sub + 1], transforms,  inds_test)\n",
        "        val_dataloader = DataLoader(val_dataset, batch_size=args.batch_size, num_workers=1,\n",
        "                              pin_memory=True, shuffle=False, drop_last=False)\n",
        "        device  = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        model = get_model(batch_norm = True, reg_extractor = True, ass_extractor = True)\n",
        "        model.apply(initialize_weights)\n",
        "        criterion = nn.CrossEntropyLoss(reduction = 'mean')#torch.nn.MSELoss()\n",
        "        #optimizer = optim.SGD(model.parameters(), lr=3e-5, momentum = 0.9)#, weight_decay=args.weight_decay)\n",
        "        optimizer = optim.Adam(model.parameters(), lr=3e-4)#, momentum = 0.9)#, weight_decay=args.weight_decay)\n",
        "        description = f'arousal_racnn_leak_{sub}_{fold}'\n",
        "        train_loop(description, 1, 30)\n",
        "       \n",
        "        model_state  = torch.load(os.path.join(args.output_dir, f\"val_{description}.tgz\"))\n",
        "        #   #model = Seq2Seq(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device)\n",
        "        model.load_state_dict(model_state['model_state_dict'])\n",
        "        acc, f1 = calculate_predictions(model, val_dataloader, type_emotion, True)\n",
        "        print(f\"f1 = {f1} acc = {acc}\")\n",
        "        acc_sub.append(acc)\n",
        "        f1_sub.append(f1)\n",
        "        print(acc, f1)\n",
        "    acc_all.append(acc_sub)     \n",
        "    f1_all.append(f1_sub)     \n",
        "    pd.DataFrame(f1_all).to_csv(\"f1_arousal_racnn_leak_result2.csv\")\n",
        "    pd.DataFrame(acc_all).to_csv(\"acc_arousal_racnn_leak_result2.csv\")\n",
        "\n",
        "\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*******18*********\n",
            "fold = 0\n",
            "0.93125\n",
            "[[105  27]\n",
            " [  6 342]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.80      0.86       132\n",
            "           1       0.93      0.98      0.95       348\n",
            "\n",
            "    accuracy                           0.93       480\n",
            "   macro avg       0.94      0.89      0.91       480\n",
            "weighted avg       0.93      0.93      0.93       480\n",
            "\n",
            "f1 = 0.9090862131308435 acc = 0.93125\n",
            "0.93125 0.9090862131308435\n",
            "fold = 1\n",
            "0.94375\n",
            "[[114  18]\n",
            " [  9 339]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.86      0.89       132\n",
            "           1       0.95      0.97      0.96       348\n",
            "\n",
            "    accuracy                           0.94       480\n",
            "   macro avg       0.94      0.92      0.93       480\n",
            "weighted avg       0.94      0.94      0.94       480\n",
            "\n",
            "f1 = 0.9279098873591989 acc = 0.94375\n",
            "0.94375 0.9279098873591989\n",
            "fold = 2\n",
            "0.9583333333333334\n",
            "[[118  14]\n",
            " [  6 342]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.89      0.92       132\n",
            "           1       0.96      0.98      0.97       348\n",
            "\n",
            "    accuracy                           0.96       480\n",
            "   macro avg       0.96      0.94      0.95       480\n",
            "weighted avg       0.96      0.96      0.96       480\n",
            "\n",
            "f1 = 0.9467329545454546 acc = 0.9583333333333334\n",
            "0.9583333333333334 0.9467329545454546\n",
            "fold = 3\n",
            "0.9604166666666667\n",
            "[[122  10]\n",
            " [  9 339]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.92      0.93       132\n",
            "           1       0.97      0.97      0.97       348\n",
            "\n",
            "    accuracy                           0.96       480\n",
            "   macro avg       0.95      0.95      0.95       480\n",
            "weighted avg       0.96      0.96      0.96       480\n",
            "\n",
            "f1 = 0.9502484848154229 acc = 0.9604166666666667\n",
            "0.9604166666666667 0.9502484848154229\n",
            "fold = 4\n",
            "0.96875\n",
            "[[122  10]\n",
            " [  5 343]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.92      0.94       132\n",
            "           1       0.97      0.99      0.98       348\n",
            "\n",
            "    accuracy                           0.97       480\n",
            "   macro avg       0.97      0.95      0.96       480\n",
            "weighted avg       0.97      0.97      0.97       480\n",
            "\n",
            "f1 = 0.9603434696159374 acc = 0.96875\n",
            "0.96875 0.9603434696159374\n",
            "*******19*********\n",
            "fold = 0\n",
            "0.9979166666666667\n",
            "[[288   0]\n",
            " [  1 191]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       288\n",
            "           1       1.00      0.99      1.00       192\n",
            "\n",
            "    accuracy                           1.00       480\n",
            "   macro avg       1.00      1.00      1.00       480\n",
            "weighted avg       1.00      1.00      1.00       480\n",
            "\n",
            "f1 = 0.9978279658447629 acc = 0.9979166666666667\n",
            "0.9979166666666667 0.9978279658447629\n",
            "fold = 1\n",
            "1.0\n",
            "[[288   0]\n",
            " [  0 192]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       288\n",
            "           1       1.00      1.00      1.00       192\n",
            "\n",
            "    accuracy                           1.00       480\n",
            "   macro avg       1.00      1.00      1.00       480\n",
            "weighted avg       1.00      1.00      1.00       480\n",
            "\n",
            "f1 = 1.0 acc = 1.0\n",
            "1.0 1.0\n",
            "fold = 2\n",
            "1.0\n",
            "[[288   0]\n",
            " [  0 192]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       288\n",
            "           1       1.00      1.00      1.00       192\n",
            "\n",
            "    accuracy                           1.00       480\n",
            "   macro avg       1.00      1.00      1.00       480\n",
            "weighted avg       1.00      1.00      1.00       480\n",
            "\n",
            "f1 = 1.0 acc = 1.0\n",
            "1.0 1.0\n",
            "fold = 3\n",
            "1.0\n",
            "[[288   0]\n",
            " [  0 192]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       288\n",
            "           1       1.00      1.00      1.00       192\n",
            "\n",
            "    accuracy                           1.00       480\n",
            "   macro avg       1.00      1.00      1.00       480\n",
            "weighted avg       1.00      1.00      1.00       480\n",
            "\n",
            "f1 = 1.0 acc = 1.0\n",
            "1.0 1.0\n",
            "fold = 4\n",
            "0.9958333333333333\n",
            "[[288   0]\n",
            " [  2 190]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      1.00       288\n",
            "           1       1.00      0.99      0.99       192\n",
            "\n",
            "    accuracy                           1.00       480\n",
            "   macro avg       1.00      0.99      1.00       480\n",
            "weighted avg       1.00      1.00      1.00       480\n",
            "\n",
            "f1 = 0.9956520951466512 acc = 0.9958333333333333\n",
            "0.9958333333333333 0.9956520951466512\n",
            "*******20*********\n",
            "fold = 0\n",
            "0.94375\n",
            "[[ 33  27]\n",
            " [  0 420]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.55      0.71        60\n",
            "           1       0.94      1.00      0.97       420\n",
            "\n",
            "    accuracy                           0.94       480\n",
            "   macro avg       0.97      0.78      0.84       480\n",
            "weighted avg       0.95      0.94      0.94       480\n",
            "\n",
            "f1 = 0.839267775421364 acc = 0.94375\n",
            "0.94375 0.839267775421364\n",
            "fold = 1\n",
            "0.9333333333333333\n",
            "[[ 29  31]\n",
            " [  1 419]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.48      0.64        60\n",
            "           1       0.93      1.00      0.96       420\n",
            "\n",
            "    accuracy                           0.93       480\n",
            "   macro avg       0.95      0.74      0.80       480\n",
            "weighted avg       0.94      0.93      0.92       480\n",
            "\n",
            "f1 = 0.8038314176245211 acc = 0.9333333333333333\n",
            "0.9333333333333333 0.8038314176245211\n",
            "fold = 2\n",
            "0.9520833333333333\n",
            "[[ 41  19]\n",
            " [  4 416]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.68      0.78        60\n",
            "           1       0.96      0.99      0.97       420\n",
            "\n",
            "    accuracy                           0.95       480\n",
            "   macro avg       0.93      0.84      0.88       480\n",
            "weighted avg       0.95      0.95      0.95       480\n",
            "\n",
            "f1 = 0.8770258980785296 acc = 0.9520833333333333\n",
            "0.9520833333333333 0.8770258980785296\n",
            "fold = 3\n",
            "0.9104166666666667\n",
            "[[ 17  43]\n",
            " [  0 420]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.28      0.44        60\n",
            "           1       0.91      1.00      0.95       420\n",
            "\n",
            "    accuracy                           0.91       480\n",
            "   macro avg       0.95      0.64      0.70       480\n",
            "weighted avg       0.92      0.91      0.89       480\n",
            "\n",
            "f1 = 0.6964304099071936 acc = 0.9104166666666667\n",
            "0.9104166666666667 0.6964304099071936\n",
            "fold = 4\n",
            "0.9375\n",
            "[[ 31  29]\n",
            " [  1 419]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.52      0.67        60\n",
            "           1       0.94      1.00      0.97       420\n",
            "\n",
            "    accuracy                           0.94       480\n",
            "   macro avg       0.95      0.76      0.82       480\n",
            "weighted avg       0.94      0.94      0.93       480\n",
            "\n",
            "f1 = 0.819675415748347 acc = 0.9375\n",
            "0.9375 0.819675415748347\n",
            "*******21*********\n",
            "fold = 0\n",
            "0.9875\n",
            "[[102   6]\n",
            " [  0 372]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.94      0.97       108\n",
            "           1       0.98      1.00      0.99       372\n",
            "\n",
            "    accuracy                           0.99       480\n",
            "   macro avg       0.99      0.97      0.98       480\n",
            "weighted avg       0.99      0.99      0.99       480\n",
            "\n",
            "f1 = 0.9817142857142857 acc = 0.9875\n",
            "0.9875 0.9817142857142857\n",
            "fold = 1\n",
            "0.9854166666666667\n",
            "[[101   7]\n",
            " [  0 372]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.94      0.97       108\n",
            "           1       0.98      1.00      0.99       372\n",
            "\n",
            "    accuracy                           0.99       480\n",
            "   macro avg       0.99      0.97      0.98       480\n",
            "weighted avg       0.99      0.99      0.99       480\n",
            "\n",
            "f1 = 0.9785931357870526 acc = 0.9854166666666667\n",
            "0.9854166666666667 0.9785931357870526\n",
            "fold = 2\n",
            "0.9520833333333333\n",
            "[[ 91  17]\n",
            " [  6 366]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.84      0.89       108\n",
            "           1       0.96      0.98      0.97       372\n",
            "\n",
            "    accuracy                           0.95       480\n",
            "   macro avg       0.95      0.91      0.93       480\n",
            "weighted avg       0.95      0.95      0.95       480\n",
            "\n",
            "f1 = 0.92867065094492 acc = 0.9520833333333333\n",
            "0.9520833333333333 0.92867065094492\n",
            "fold = 3\n",
            "0.9791666666666666\n",
            "[[102   6]\n",
            " [  4 368]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.94      0.95       108\n",
            "           1       0.98      0.99      0.99       372\n",
            "\n",
            "    accuracy                           0.98       480\n",
            "   macro avg       0.97      0.97      0.97       480\n",
            "weighted avg       0.98      0.98      0.98       480\n",
            "\n",
            "f1 = 0.9699331011500589 acc = 0.9791666666666666\n",
            "0.9791666666666666 0.9699331011500589\n",
            "fold = 4\n",
            "0.96875\n",
            "[[ 95  13]\n",
            " [  2 370]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.88      0.93       108\n",
            "           1       0.97      0.99      0.98       372\n",
            "\n",
            "    accuracy                           0.97       480\n",
            "   macro avg       0.97      0.94      0.95       480\n",
            "weighted avg       0.97      0.97      0.97       480\n",
            "\n",
            "f1 = 0.9534808593119044 acc = 0.96875\n",
            "0.96875 0.9534808593119044\n",
            "*******22*********\n",
            "fold = 0\n",
            "0.9333333333333333\n",
            "[[217  23]\n",
            " [  9 231]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.90      0.93       240\n",
            "           1       0.91      0.96      0.94       240\n",
            "\n",
            "    accuracy                           0.93       480\n",
            "   macro avg       0.93      0.93      0.93       480\n",
            "weighted avg       0.93      0.93      0.93       480\n",
            "\n",
            "f1 = 0.9332765720838909 acc = 0.9333333333333333\n",
            "0.9333333333333333 0.9332765720838909\n",
            "fold = 1\n",
            "0.9833333333333333\n",
            "[[238   2]\n",
            " [  6 234]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.99      0.98       240\n",
            "           1       0.99      0.97      0.98       240\n",
            "\n",
            "    accuracy                           0.98       480\n",
            "   macro avg       0.98      0.98      0.98       480\n",
            "weighted avg       0.98      0.98      0.98       480\n",
            "\n",
            "f1 = 0.9833321758455449 acc = 0.9833333333333333\n",
            "0.9833333333333333 0.9833321758455449\n",
            "fold = 2\n",
            "0.9708333333333333\n",
            "[[230  10]\n",
            " [  4 236]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97       240\n",
            "           1       0.96      0.98      0.97       240\n",
            "\n",
            "    accuracy                           0.97       480\n",
            "   macro avg       0.97      0.97      0.97       480\n",
            "weighted avg       0.97      0.97      0.97       480\n",
            "\n",
            "f1 = 0.9708287753294784 acc = 0.9708333333333333\n",
            "0.9708333333333333 0.9708287753294784\n",
            "fold = 3\n",
            "0.9833333333333333\n",
            "[[236   4]\n",
            " [  4 236]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.98      0.98       240\n",
            "           1       0.98      0.98      0.98       240\n",
            "\n",
            "    accuracy                           0.98       480\n",
            "   macro avg       0.98      0.98      0.98       480\n",
            "weighted avg       0.98      0.98      0.98       480\n",
            "\n",
            "f1 = 0.9833333333333333 acc = 0.9833333333333333\n",
            "0.9833333333333333 0.9833333333333333\n",
            "fold = 4\n",
            "0.9583333333333334\n",
            "[[231   9]\n",
            " [ 11 229]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.96      0.96       240\n",
            "           1       0.96      0.95      0.96       240\n",
            "\n",
            "    accuracy                           0.96       480\n",
            "   macro avg       0.96      0.96      0.96       480\n",
            "weighted avg       0.96      0.96      0.96       480\n",
            "\n",
            "f1 = 0.9583326099411449 acc = 0.9583333333333334\n",
            "0.9583333333333334 0.9583326099411449\n",
            "*******23*********\n",
            "fold = 0\n",
            "0.99375\n",
            "[[142   2]\n",
            " [  1 335]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.99      0.99       144\n",
            "           1       0.99      1.00      1.00       336\n",
            "\n",
            "    accuracy                           0.99       480\n",
            "   macro avg       0.99      0.99      0.99       480\n",
            "weighted avg       0.99      0.99      0.99       480\n",
            "\n",
            "f1 = 0.9925446930122029 acc = 0.99375\n",
            "0.99375 0.9925446930122029\n",
            "fold = 1\n",
            "1.0\n",
            "[[144   0]\n",
            " [  0 336]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       144\n",
            "           1       1.00      1.00      1.00       336\n",
            "\n",
            "    accuracy                           1.00       480\n",
            "   macro avg       1.00      1.00      1.00       480\n",
            "weighted avg       1.00      1.00      1.00       480\n",
            "\n",
            "f1 = 1.0 acc = 1.0\n",
            "1.0 1.0\n",
            "fold = 2\n",
            "0.9958333333333333\n",
            "[[144   0]\n",
            " [  2 334]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99       144\n",
            "           1       1.00      0.99      1.00       336\n",
            "\n",
            "    accuracy                           1.00       480\n",
            "   macro avg       0.99      1.00      1.00       480\n",
            "weighted avg       1.00      1.00      1.00       480\n",
            "\n",
            "f1 = 0.9950591868244982 acc = 0.9958333333333333\n",
            "0.9958333333333333 0.9950591868244982\n",
            "fold = 3\n",
            "1.0\n",
            "[[144   0]\n",
            " [  0 336]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       144\n",
            "           1       1.00      1.00      1.00       336\n",
            "\n",
            "    accuracy                           1.00       480\n",
            "   macro avg       1.00      1.00      1.00       480\n",
            "weighted avg       1.00      1.00      1.00       480\n",
            "\n",
            "f1 = 1.0 acc = 1.0\n",
            "1.0 1.0\n",
            "fold = 4\n",
            "0.9979166666666667\n",
            "[[144   0]\n",
            " [  1 335]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      1.00       144\n",
            "           1       1.00      1.00      1.00       336\n",
            "\n",
            "    accuracy                           1.00       480\n",
            "   macro avg       1.00      1.00      1.00       480\n",
            "weighted avg       1.00      1.00      1.00       480\n",
            "\n",
            "f1 = 0.9975247397109102 acc = 0.9979166666666667\n",
            "0.9979166666666667 0.9975247397109102\n",
            "*******24*********\n",
            "fold = 0\n",
            "0.9604166666666667\n",
            "[[237   3]\n",
            " [ 16 224]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.99      0.96       240\n",
            "           1       0.99      0.93      0.96       240\n",
            "\n",
            "    accuracy                           0.96       480\n",
            "   macro avg       0.96      0.96      0.96       480\n",
            "weighted avg       0.96      0.96      0.96       480\n",
            "\n",
            "f1 = 0.9603876107040321 acc = 0.9604166666666667\n",
            "0.9604166666666667 0.9603876107040321\n",
            "fold = 1\n",
            "0.9729166666666667\n",
            "[[233   7]\n",
            " [  6 234]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.97      0.97       240\n",
            "           1       0.97      0.97      0.97       240\n",
            "\n",
            "    accuracy                           0.97       480\n",
            "   macro avg       0.97      0.97      0.97       480\n",
            "weighted avg       0.97      0.97      0.97       480\n",
            "\n",
            "f1 = 0.9729165491169666 acc = 0.9729166666666667\n",
            "0.9729166666666667 0.9729165491169666\n",
            "fold = 2\n",
            "0.8166666666666667\n",
            "[[188  52]\n",
            " [ 36 204]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.78      0.81       240\n",
            "           1       0.80      0.85      0.82       240\n",
            "\n",
            "    accuracy                           0.82       480\n",
            "   macro avg       0.82      0.82      0.82       480\n",
            "weighted avg       0.82      0.82      0.82       480\n",
            "\n",
            "f1 = 0.8164627363737486 acc = 0.8166666666666667\n",
            "0.8166666666666667 0.8164627363737486\n",
            "fold = 3\n",
            "0.9479166666666666\n",
            "[[230  10]\n",
            " [ 15 225]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.96      0.95       240\n",
            "           1       0.96      0.94      0.95       240\n",
            "\n",
            "    accuracy                           0.95       480\n",
            "   macro avg       0.95      0.95      0.95       480\n",
            "weighted avg       0.95      0.95      0.95       480\n",
            "\n",
            "f1 = 0.9479110146500271 acc = 0.9479166666666666\n",
            "0.9479166666666666 0.9479110146500271\n",
            "fold = 4\n",
            "0.9791666666666666\n",
            "[[233   7]\n",
            " [  3 237]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98       240\n",
            "           1       0.97      0.99      0.98       240\n",
            "\n",
            "    accuracy                           0.98       480\n",
            "   macro avg       0.98      0.98      0.98       480\n",
            "weighted avg       0.98      0.98      0.98       480\n",
            "\n",
            "f1 = 0.9791652198069309 acc = 0.9791666666666666\n",
            "0.9791666666666666 0.9791652198069309\n",
            "*******25*********\n",
            "fold = 0\n",
            "0.9979166666666667\n",
            "[[155   1]\n",
            " [  0 324]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.99      1.00       156\n",
            "           1       1.00      1.00      1.00       324\n",
            "\n",
            "    accuracy                           1.00       480\n",
            "   macro avg       1.00      1.00      1.00       480\n",
            "weighted avg       1.00      1.00      1.00       480\n",
            "\n",
            "f1 = 0.997621866933546 acc = 0.9979166666666667\n",
            "0.9979166666666667 0.997621866933546\n",
            "fold = 1\n",
            "0.99375\n",
            "[[154   2]\n",
            " [  1 323]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.99      0.99       156\n",
            "           1       0.99      1.00      1.00       324\n",
            "\n",
            "    accuracy                           0.99       480\n",
            "   macro avg       0.99      0.99      0.99       480\n",
            "weighted avg       0.99      0.99      0.99       480\n",
            "\n",
            "f1 = 0.992865600800638 acc = 0.99375\n",
            "0.99375 0.992865600800638\n",
            "fold = 2\n",
            "0.9979166666666667\n",
            "[[156   0]\n",
            " [  1 323]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      1.00       156\n",
            "           1       1.00      1.00      1.00       324\n",
            "\n",
            "    accuracy                           1.00       480\n",
            "   macro avg       1.00      1.00      1.00       480\n",
            "weighted avg       1.00      1.00      1.00       480\n",
            "\n",
            "f1 = 0.9976297583834952 acc = 0.9979166666666667\n",
            "0.9979166666666667 0.9976297583834952\n",
            "fold = 3\n",
            "1.0\n",
            "[[156   0]\n",
            " [  0 324]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       156\n",
            "           1       1.00      1.00      1.00       324\n",
            "\n",
            "    accuracy                           1.00       480\n",
            "   macro avg       1.00      1.00      1.00       480\n",
            "weighted avg       1.00      1.00      1.00       480\n",
            "\n",
            "f1 = 1.0 acc = 1.0\n",
            "1.0 1.0\n",
            "fold = 4\n",
            "0.9916666666666667\n",
            "[[154   2]\n",
            " [  2 322]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.99      0.99       156\n",
            "           1       0.99      0.99      0.99       324\n",
            "\n",
            "    accuracy                           0.99       480\n",
            "   macro avg       0.99      0.99      0.99       480\n",
            "weighted avg       0.99      0.99      0.99       480\n",
            "\n",
            "f1 = 0.9905033238366572 acc = 0.9916666666666667\n",
            "0.9916666666666667 0.9905033238366572\n",
            "*******26*********\n",
            "fold = 0\n",
            "1.0\n",
            "[[180   0]\n",
            " [  0 300]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       180\n",
            "           1       1.00      1.00      1.00       300\n",
            "\n",
            "    accuracy                           1.00       480\n",
            "   macro avg       1.00      1.00      1.00       480\n",
            "weighted avg       1.00      1.00      1.00       480\n",
            "\n",
            "f1 = 1.0 acc = 1.0\n",
            "1.0 1.0\n",
            "fold = 1\n",
            "0.9916666666666667\n",
            "[[178   2]\n",
            " [  2 298]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.99      0.99       180\n",
            "           1       0.99      0.99      0.99       300\n",
            "\n",
            "    accuracy                           0.99       480\n",
            "   macro avg       0.99      0.99      0.99       480\n",
            "weighted avg       0.99      0.99      0.99       480\n",
            "\n",
            "f1 = 0.991111111111111 acc = 0.9916666666666667\n",
            "0.9916666666666667 0.991111111111111\n",
            "fold = 2\n",
            "1.0\n",
            "[[180   0]\n",
            " [  0 300]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       180\n",
            "           1       1.00      1.00      1.00       300\n",
            "\n",
            "    accuracy                           1.00       480\n",
            "   macro avg       1.00      1.00      1.00       480\n",
            "weighted avg       1.00      1.00      1.00       480\n",
            "\n",
            "f1 = 1.0 acc = 1.0\n",
            "1.0 1.0\n",
            "fold = 3\n",
            "0.99375\n",
            "[[180   0]\n",
            " [  3 297]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      1.00      0.99       180\n",
            "           1       1.00      0.99      0.99       300\n",
            "\n",
            "    accuracy                           0.99       480\n",
            "   macro avg       0.99      0.99      0.99       480\n",
            "weighted avg       0.99      0.99      0.99       480\n",
            "\n",
            "f1 = 0.9933552057809709 acc = 0.99375\n",
            "0.99375 0.9933552057809709\n",
            "fold = 4\n",
            "1.0\n",
            "[[180   0]\n",
            " [  0 300]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       180\n",
            "           1       1.00      1.00      1.00       300\n",
            "\n",
            "    accuracy                           1.00       480\n",
            "   macro avg       1.00      1.00      1.00       480\n",
            "weighted avg       1.00      1.00      1.00       480\n",
            "\n",
            "f1 = 1.0 acc = 1.0\n",
            "1.0 1.0\n",
            "*******27*********\n",
            "fold = 0\n",
            "1.0\n",
            "[[228   0]\n",
            " [  0 252]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       228\n",
            "           1       1.00      1.00      1.00       252\n",
            "\n",
            "    accuracy                           1.00       480\n",
            "   macro avg       1.00      1.00      1.00       480\n",
            "weighted avg       1.00      1.00      1.00       480\n",
            "\n",
            "f1 = 1.0 acc = 1.0\n",
            "1.0 1.0\n",
            "fold = 1\n",
            "1.0\n",
            "[[228   0]\n",
            " [  0 252]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       228\n",
            "           1       1.00      1.00      1.00       252\n",
            "\n",
            "    accuracy                           1.00       480\n",
            "   macro avg       1.00      1.00      1.00       480\n",
            "weighted avg       1.00      1.00      1.00       480\n",
            "\n",
            "f1 = 1.0 acc = 1.0\n",
            "1.0 1.0\n",
            "fold = 2\n",
            "1.0\n",
            "[[228   0]\n",
            " [  0 252]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       228\n",
            "           1       1.00      1.00      1.00       252\n",
            "\n",
            "    accuracy                           1.00       480\n",
            "   macro avg       1.00      1.00      1.00       480\n",
            "weighted avg       1.00      1.00      1.00       480\n",
            "\n",
            "f1 = 1.0 acc = 1.0\n",
            "1.0 1.0\n",
            "fold = 3\n",
            "0.9958333333333333\n",
            "[[228   0]\n",
            " [  2 250]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      1.00       228\n",
            "           1       1.00      0.99      1.00       252\n",
            "\n",
            "    accuracy                           1.00       480\n",
            "   macro avg       1.00      1.00      1.00       480\n",
            "weighted avg       1.00      1.00      1.00       480\n",
            "\n",
            "f1 = 0.995824562013953 acc = 0.9958333333333333\n",
            "0.9958333333333333 0.995824562013953\n",
            "fold = 4\n",
            "0.99375\n",
            "[[226   2]\n",
            " [  1 251]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.99      0.99       228\n",
            "           1       0.99      1.00      0.99       252\n",
            "\n",
            "    accuracy                           0.99       480\n",
            "   macro avg       0.99      0.99      0.99       480\n",
            "weighted avg       0.99      0.99      0.99       480\n",
            "\n",
            "f1 = 0.9937329996735937 acc = 0.99375\n",
            "0.99375 0.9937329996735937\n",
            "*******28*********\n",
            "fold = 0\n",
            "0.9833333333333333\n",
            "[[126   6]\n",
            " [  2 346]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.97       132\n",
            "           1       0.98      0.99      0.99       348\n",
            "\n",
            "    accuracy                           0.98       480\n",
            "   macro avg       0.98      0.97      0.98       480\n",
            "weighted avg       0.98      0.98      0.98       480\n",
            "\n",
            "f1 = 0.9789010989010989 acc = 0.9833333333333333\n",
            "0.9833333333333333 0.9789010989010989\n",
            "fold = 1\n",
            "0.975\n",
            "[[123   9]\n",
            " [  3 345]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.93      0.95       132\n",
            "           1       0.97      0.99      0.98       348\n",
            "\n",
            "    accuracy                           0.97       480\n",
            "   macro avg       0.98      0.96      0.97       480\n",
            "weighted avg       0.98      0.97      0.97       480\n",
            "\n",
            "f1 = 0.9681971774995031 acc = 0.975\n",
            "0.975 0.9681971774995031\n",
            "fold = 2\n",
            "0.9708333333333333\n",
            "[[121  11]\n",
            " [  3 345]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.92      0.95       132\n",
            "           1       0.97      0.99      0.98       348\n",
            "\n",
            "    accuracy                           0.97       480\n",
            "   macro avg       0.97      0.95      0.96       480\n",
            "weighted avg       0.97      0.97      0.97       480\n",
            "\n",
            "f1 = 0.9627130681818181 acc = 0.9708333333333333\n",
            "0.9708333333333333 0.9627130681818181\n",
            "fold = 3\n",
            "0.9875\n",
            "[[126   6]\n",
            " [  0 348]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.95      0.98       132\n",
            "           1       0.98      1.00      0.99       348\n",
            "\n",
            "    accuracy                           0.99       480\n",
            "   macro avg       0.99      0.98      0.98       480\n",
            "weighted avg       0.99      0.99      0.99       480\n",
            "\n",
            "f1 = 0.9840985887497515 acc = 0.9875\n",
            "0.9875 0.9840985887497515\n",
            "fold = 4\n",
            "0.94375\n",
            "[[109  23]\n",
            " [  4 344]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.83      0.89       132\n",
            "           1       0.94      0.99      0.96       348\n",
            "\n",
            "    accuracy                           0.94       480\n",
            "   macro avg       0.95      0.91      0.93       480\n",
            "weighted avg       0.94      0.94      0.94       480\n",
            "\n",
            "f1 = 0.9260168403025546 acc = 0.94375\n",
            "0.94375 0.9260168403025546\n",
            "*******29*********\n",
            "fold = 0\n",
            "0.9854166666666667\n",
            "[[372   0]\n",
            " [  7 101]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      1.00      0.99       372\n",
            "           1       1.00      0.94      0.97       108\n",
            "\n",
            "    accuracy                           0.99       480\n",
            "   macro avg       0.99      0.97      0.98       480\n",
            "weighted avg       0.99      0.99      0.99       480\n",
            "\n",
            "f1 = 0.9785931357870526 acc = 0.9854166666666667\n",
            "0.9854166666666667 0.9785931357870526\n",
            "fold = 1\n",
            "0.9875\n",
            "[[372   0]\n",
            " [  6 102]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      1.00      0.99       372\n",
            "           1       1.00      0.94      0.97       108\n",
            "\n",
            "    accuracy                           0.99       480\n",
            "   macro avg       0.99      0.97      0.98       480\n",
            "weighted avg       0.99      0.99      0.99       480\n",
            "\n",
            "f1 = 0.9817142857142857 acc = 0.9875\n",
            "0.9875 0.9817142857142857\n",
            "fold = 2\n",
            "0.9916666666666667\n",
            "[[371   1]\n",
            " [  3 105]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99       372\n",
            "           1       0.99      0.97      0.98       108\n",
            "\n",
            "    accuracy                           0.99       480\n",
            "   macro avg       0.99      0.98      0.99       480\n",
            "weighted avg       0.99      0.99      0.99       480\n",
            "\n",
            "f1 = 0.9879732404600235 acc = 0.9916666666666667\n",
            "0.9916666666666667 0.9879732404600235\n",
            "fold = 3\n",
            "0.99375\n",
            "[[370   2]\n",
            " [  1 107]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.99      1.00       372\n",
            "           1       0.98      0.99      0.99       108\n",
            "\n",
            "    accuracy                           0.99       480\n",
            "   macro avg       0.99      0.99      0.99       480\n",
            "weighted avg       0.99      0.99      0.99       480\n",
            "\n",
            "f1 = 0.991068715073404 acc = 0.99375\n",
            "0.99375 0.991068715073404\n",
            "fold = 4\n",
            "0.9916666666666667\n",
            "[[371   1]\n",
            " [  3 105]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99       372\n",
            "           1       0.99      0.97      0.98       108\n",
            "\n",
            "    accuracy                           0.99       480\n",
            "   macro avg       0.99      0.98      0.99       480\n",
            "weighted avg       0.99      0.99      0.99       480\n",
            "\n",
            "f1 = 0.9879732404600235 acc = 0.9916666666666667\n",
            "0.9916666666666667 0.9879732404600235\n",
            "*******30*********\n",
            "fold = 0\n",
            "0.8375\n",
            "[[228  36]\n",
            " [ 42 174]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.86      0.85       264\n",
            "           1       0.83      0.81      0.82       216\n",
            "\n",
            "    accuracy                           0.84       480\n",
            "   macro avg       0.84      0.83      0.84       480\n",
            "weighted avg       0.84      0.84      0.84       480\n",
            "\n",
            "f1 = 0.8354169963601836 acc = 0.8375\n",
            "0.8375 0.8354169963601836\n",
            "fold = 1\n",
            "0.8104166666666667\n",
            "[[233  31]\n",
            " [ 60 156]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.88      0.84       264\n",
            "           1       0.83      0.72      0.77       216\n",
            "\n",
            "    accuracy                           0.81       480\n",
            "   macro avg       0.81      0.80      0.81       480\n",
            "weighted avg       0.81      0.81      0.81       480\n",
            "\n",
            "f1 = 0.8054091619852899 acc = 0.8104166666666667\n",
            "0.8104166666666667 0.8054091619852899\n",
            "fold = 2\n",
            "0.8729166666666667\n",
            "[[238  26]\n",
            " [ 35 181]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.90      0.89       264\n",
            "           1       0.87      0.84      0.86       216\n",
            "\n",
            "    accuracy                           0.87       480\n",
            "   macro avg       0.87      0.87      0.87       480\n",
            "weighted avg       0.87      0.87      0.87       480\n",
            "\n",
            "f1 = 0.8710989606032992 acc = 0.8729166666666667\n",
            "0.8729166666666667 0.8710989606032992\n",
            "fold = 3\n",
            "0.8104166666666667\n",
            "[[229  35]\n",
            " [ 56 160]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.87      0.83       264\n",
            "           1       0.82      0.74      0.78       216\n",
            "\n",
            "    accuracy                           0.81       480\n",
            "   macro avg       0.81      0.80      0.81       480\n",
            "weighted avg       0.81      0.81      0.81       480\n",
            "\n",
            "f1 = 0.8064164439658038 acc = 0.8104166666666667\n",
            "0.8104166666666667 0.8064164439658038\n",
            "fold = 4\n",
            "0.8145833333333333\n",
            "[[216  48]\n",
            " [ 41 175]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.82      0.83       264\n",
            "           1       0.78      0.81      0.80       216\n",
            "\n",
            "    accuracy                           0.81       480\n",
            "   macro avg       0.81      0.81      0.81       480\n",
            "weighted avg       0.82      0.81      0.81       480\n",
            "\n",
            "f1 = 0.813220589456932 acc = 0.8145833333333333\n",
            "0.8145833333333333 0.813220589456932\n",
            "*******31*********\n",
            "fold = 0\n",
            "0.7583333333333333\n",
            "[[ 87  69]\n",
            " [ 47 277]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.56      0.60       156\n",
            "           1       0.80      0.85      0.83       324\n",
            "\n",
            "    accuracy                           0.76       480\n",
            "   macro avg       0.72      0.71      0.71       480\n",
            "weighted avg       0.75      0.76      0.75       480\n",
            "\n",
            "f1 = 0.7134328358208955 acc = 0.7583333333333333\n",
            "0.7583333333333333 0.7134328358208955\n",
            "fold = 1\n",
            "0.74375\n",
            "[[ 78  78]\n",
            " [ 45 279]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.50      0.56       156\n",
            "           1       0.78      0.86      0.82       324\n",
            "\n",
            "    accuracy                           0.74       480\n",
            "   macro avg       0.71      0.68      0.69       480\n",
            "weighted avg       0.73      0.74      0.73       480\n",
            "\n",
            "f1 = 0.6892615224290655 acc = 0.74375\n",
            "0.74375 0.6892615224290655\n",
            "fold = 2\n",
            "0.7520833333333333\n",
            "[[ 62  94]\n",
            " [ 25 299]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.40      0.51       156\n",
            "           1       0.76      0.92      0.83       324\n",
            "\n",
            "    accuracy                           0.75       480\n",
            "   macro avg       0.74      0.66      0.67       480\n",
            "weighted avg       0.75      0.75      0.73       480\n",
            "\n",
            "f1 = 0.6721593746233449 acc = 0.7520833333333333\n",
            "0.7520833333333333 0.6721593746233449\n",
            "fold = 3\n",
            "0.81875\n",
            "[[108  48]\n",
            " [ 39 285]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.69      0.71       156\n",
            "           1       0.86      0.88      0.87       324\n",
            "\n",
            "    accuracy                           0.82       480\n",
            "   macro avg       0.80      0.79      0.79       480\n",
            "weighted avg       0.82      0.82      0.82       480\n",
            "\n",
            "f1 = 0.7902255979022559 acc = 0.81875\n",
            "0.81875 0.7902255979022559\n",
            "fold = 4\n",
            "0.74375\n",
            "[[ 74  82]\n",
            " [ 41 283]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.64      0.47      0.55       156\n",
            "           1       0.78      0.87      0.82       324\n",
            "\n",
            "    accuracy                           0.74       480\n",
            "   macro avg       0.71      0.67      0.68       480\n",
            "weighted avg       0.73      0.74      0.73       480\n",
            "\n",
            "f1 = 0.6838029338203397 acc = 0.74375\n",
            "0.74375 0.6838029338203397\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f1_all)"
      ],
      "metadata": {
        "id": "L7qz6jLCJ9gI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b047762e-6967-4ee9-9d26-a836030b7ee3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.9090862131308435, 0.9279098873591989, 0.9467329545454546, 0.9502484848154229, 0.9603434696159374], [0.9978279658447629, 1.0, 1.0, 1.0, 0.9956520951466512], [0.839267775421364, 0.8038314176245211, 0.8770258980785296, 0.6964304099071936, 0.819675415748347], [0.9817142857142857, 0.9785931357870526, 0.92867065094492, 0.9699331011500589, 0.9534808593119044], [0.9332765720838909, 0.9833321758455449, 0.9708287753294784, 0.9833333333333333, 0.9583326099411449], [0.9925446930122029, 1.0, 0.9950591868244982, 1.0, 0.9975247397109102], [0.9603876107040321, 0.9729165491169666, 0.8164627363737486, 0.9479110146500271, 0.9791652198069309], [0.997621866933546, 0.992865600800638, 0.9976297583834952, 1.0, 0.9905033238366572], [1.0, 0.991111111111111, 1.0, 0.9933552057809709, 1.0], [1.0, 1.0, 1.0, 0.995824562013953, 0.9937329996735937], [0.9789010989010989, 0.9681971774995031, 0.9627130681818181, 0.9840985887497515, 0.9260168403025546], [0.9785931357870526, 0.9817142857142857, 0.9879732404600235, 0.991068715073404, 0.9879732404600235], [0.8354169963601836, 0.8054091619852899, 0.8710989606032992, 0.8064164439658038, 0.813220589456932], [0.7134328358208955, 0.6892615224290655, 0.6721593746233449, 0.7902255979022559, 0.6838029338203397]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "f1_all1 = pd.read_csv(\"f1_arousal_racnn_leak_result_d.csv\")\n",
        "acc_all1 = pd.read_csv(\"acc_arousal_racnn_leak_result_d.csv\")\n",
        "f1_all2 = pd.read_csv(\"f1_arousal_racnn_leak_result2.csv\")\n",
        "acc_all2 = pd.read_csv(\"acc_arousal_racnn_leak_result2.csv\")"
      ],
      "metadata": {
        "id": "K7Bz35BnXCbz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1_all = pd.concat((f1_all1, f1_all2), axis = 0, ignore_index = True).reset_index()\n",
        "acc_all = pd.concat((acc_all1, acc_all2), axis = 0)"
      ],
      "metadata": {
        "id": "JmX9s9-LYKIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f1_all)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ArewmUTXYnUV",
        "outputId": "14f775dd-66f7-4a84-d611-a108781c0332"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    index  Unnamed: 0         0         1         2         3         4\n",
            "0       0           0  0.972665  0.978593  0.973206  0.972849  0.975946\n",
            "1       1           1  0.981248  0.997917  0.979166  0.968747  0.977083\n",
            "2       2           2  0.803831  0.868367  0.925766  0.810778  0.780093\n",
            "3       3           3  0.970833  0.979161  0.970831  0.993750  0.972907\n",
            "4       4           4  1.000000  0.997828  1.000000  0.995652  0.997828\n",
            "5       5           5  0.976089  0.970776  0.968352  0.992107  0.978693\n",
            "6       6           6  1.000000  1.000000  1.000000  1.000000  1.000000\n",
            "7       7           7  0.968765  0.995236  0.985659  0.992866  0.997622\n",
            "8       8           8  0.830010  0.841671  0.852931  0.830241  0.894498\n",
            "9       9           9  0.933906  0.914596  0.914345  0.943358  0.940607\n",
            "10     10          10  0.844667  0.919558  0.969084  0.914530  0.905754\n",
            "11     11          11  0.975302  0.868118  0.808047  0.904556  0.811834\n",
            "12     12          12  0.997896  0.997895  0.991589  0.997896  1.000000\n",
            "13     13          13  0.995565  0.997775  0.995546  0.993311  1.000000\n",
            "14     14          14  0.969618  0.973753  0.869557  0.918914  0.959659\n",
            "15     15          15  0.994026  0.984814  1.000000  0.975450  0.991010\n",
            "16     16          16  0.989350  0.991475  0.968010  0.978687  0.955214\n",
            "17     17          17  1.000000  0.997622  1.000000  0.995267  0.971014\n",
            "18     18           0  0.909086  0.927910  0.946733  0.950248  0.960343\n",
            "19     19           1  0.997828  1.000000  1.000000  1.000000  0.995652\n",
            "20     20           2  0.839268  0.803831  0.877026  0.696430  0.819675\n",
            "21     21           3  0.981714  0.978593  0.928671  0.969933  0.953481\n",
            "22     22           4  0.933277  0.983332  0.970829  0.983333  0.958333\n",
            "23     23           5  0.992545  1.000000  0.995059  1.000000  0.997525\n",
            "24     24           6  0.960388  0.972917  0.816463  0.947911  0.979165\n",
            "25     25           7  0.997622  0.992866  0.997630  1.000000  0.990503\n",
            "26     26           8  1.000000  0.991111  1.000000  0.993355  1.000000\n",
            "27     27           9  1.000000  1.000000  1.000000  0.995825  0.993733\n",
            "28     28          10  0.978901  0.968197  0.962713  0.984099  0.926017\n",
            "29     29          11  0.978593  0.981714  0.987973  0.991069  0.987973\n",
            "30     30          12  0.835417  0.805409  0.871099  0.806416  0.813221\n",
            "31     31          13  0.713433  0.689262  0.672159  0.790226  0.683803\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(f1_all).to_csv(\"f1_arousal_racnn_leak_result_article.csv\")\n",
        "pd.DataFrame(acc_all).to_csv(\"acc_arousal_racnn_leak_result_article.csv\")"
      ],
      "metadata": {
        "id": "PL5T3DmZpIzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3_qL-Tk8fQ3"
      },
      "source": [
        "acc_data = pd.DataFrame(acc_all[['0','1','2','3', '4']])\n",
        "acc_data_sub = acc_data.mean(axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1_data = pd.DataFrame(f1_all[['0','1','2','3', '4']])\n",
        "f1_data_sub = f1_data.mean(axis = 1)"
      ],
      "metadata": {
        "id": "c8K_FVzux1g2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "print(np.mean(acc_data_sub))\n",
        "print(np.mean(f1_data_sub))"
      ],
      "metadata": {
        "id": "jELRV1ShpUgu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59ebce6e-f533-41b3-f636-85007303bffc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9610286458333331\n",
            "0.9459223225001052\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45rfKagK9o_F",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "33c22d8b-99f7-4a04-932b-ca90908b03b2"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "fig, ax = plt.subplots(1, 1, figsize = (7, 2))\n",
        "ax.plot(np.arange(1,33), acc_data_sub.values)\n",
        "ax.set_title(\"Arousal, accuracy per subject\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Arousal, accuracy per subject')"
            ]
          },
          "metadata": {},
          "execution_count": 35
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 504x144 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ElOP3-g_8hs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0d29057-bfa6-4624-dd5e-15b922760b2e"
      },
      "source": [
        "print(f\"Valence accuracy = {acc_data_sub.mean()} += {acc_data_sub.std()}\" )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Valence accuracy = 0.9610286458333331 += 0.05387857469024103\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHFUys1_S-Cd"
      },
      "source": [
        "#Результаты на датасете без лика"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZW7dDHrvE59l"
      },
      "source": [
        "import glob\n",
        "import pickle\n",
        "from collections import Counter\n",
        "import scipy\n",
        "from  scipy import signal\n",
        "from scipy.fft import fft, fftfreq\n",
        "\n",
        "class EmotionDataset_balanced(Dataset):\n",
        "    def __init__ (self, indexes, data, labels_bin):  #indexes - индексы видео которые вошли в датасет, data - данные labels - метки бинарные\n",
        "       self.data = []\n",
        "       self.labels = []\n",
        "       self.cnt = [Counter(), Counter(), Counter(),Counter()]\n",
        "       self.indexes = indexes\n",
        "       self.len_files = []\n",
        "       for sub in range(len(data)): #sub   - человек\n",
        "            self.data.append(data[sub][indexes[sub], :, 3 * NTIMES_IN_SEC :  LEN_RECORD_IN_SECONDS * NTIMES_IN_SEC + 3 * NTIMES_IN_SEC])\n",
        "            self.len_files.append(len(indexes[sub]) *  LEN_RECORD_IN_SECONDS - 1) #столько записей будет храниться для каждого человека - количство видео для человека на 60 сек\n",
        "            self.len_record = LEN_RECORD_IN_SECONDS\n",
        "            labels_bin_sub = (labels_bin[sub])\n",
        "            self.labels.append(labels_bin_sub[indexes[sub]])          \n",
        "            # счетчик числа 1 и 0 в датасете\n",
        "            for type_emotion in range(4):\n",
        "              self.cnt[type_emotion].update(list(self.labels[-1][:, type_emotion]))    \n",
        "\n",
        "            \n",
        "       self.len_cumsum = np.cumsum(self.len_files)     \n",
        "\n",
        "\n",
        "    # общее число сеймплов по 1 сек в датасете. их будет 32 * количество видео в датасете * 60 \n",
        "    def __len__(self):\n",
        "        result =  sum(self.len_files) - 10\n",
        "        return result\n",
        "\n",
        "\n",
        "    # по номеру сеймпла вытягиваем номер человека номер видео и номер секунды\n",
        "    def get_index_record(self, item):\n",
        "      for sub in range(len(self.len_cumsum)):\n",
        "         #print(item, self.len_cumsum[i_file])\n",
        "         if (item > self.len_cumsum[sub]):\n",
        "            continue\n",
        "         else:\n",
        "            break\n",
        "      if sub == 0:\n",
        "         index_in_file = item\n",
        "      else:\n",
        "         index_in_file = item  - self.len_cumsum[sub - 1]\n",
        "      nvideo = index_in_file//(self.len_record)# * LEN_RECORD_IN_SECONDS *  NTIMES_IN_SEC)\n",
        "      nsec = (index_in_file - nvideo * self.len_record) # *   NTIMES_IN_SEC)\n",
        "\n",
        "      return sub, nvideo, nsec\n",
        "\n",
        "   \n",
        "    def __getitem__(self, item):\n",
        "      sample = {}\n",
        "      #print(item)\n",
        "      sub, nvideo, nsec = self.get_index_record(item)\n",
        "      #print(i_file, index_in_file, nvideo, nsec )\n",
        "      sample['data'] = np.zeros((HCANALS, WCANALS, NTIMES_IN_SAMPLE))\n",
        "      sample_from_one_canals = []\n",
        "      for i_canal in range(NCANALS):\n",
        "        sample_from_one_canal1 = self.data[sub][nvideo, i_canal, nsec * 128 : nsec * 128 + 128]\n",
        "        power = fft(sample_from_one_canal1)\n",
        "        #sample_from_one_canals.append(sample_from_one_canal)\n",
        "        sample_from_one_canals.append([0] * 128)\n",
        "        #print(sample_from_one_canal.shape)\n",
        "      sample_from_one_canals = np.asarray(sample_from_one_canals).copy()\n",
        "      # здесь делаем нормализацию фактически по поверхности головы (по одному времени t для всех каналов)\n",
        "      #sample_from_one_canals = scipy.stats.zscore(sample_from_one_canals, axis = 0)\n",
        "      for i_canal in range(NCANALS):\n",
        "          sample['data'][electrode_matrix[list_electrodes[i_canal]][0],  electrode_matrix[list_electrodes[i_canal]][1]] = sample_from_one_canals[i_canal]\n",
        "      sample['data'] = torch.FloatTensor(sample['data'])\n",
        "      sample['labels']  = torch.LongTensor(self.labels[sub][nvideo])\n",
        "      sample['position'] = torch.LongTensor((sub, nvideo, nsec))\n",
        "      \n",
        "      return sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUAmPpHwE6Bk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxxflCJDw5_G"
      },
      "source": [
        "Valence Correct Per sub model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vub7D3V6RD1"
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold \n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "#def get_mertics_per_subject():\n",
        "k  = 5\n",
        "type_emotion = 0\n",
        "acc_all = []\n",
        "f1_all = []\n",
        "\n",
        "for sub in range(0, 32):\n",
        "    print(f\"*******{sub}*********\")\n",
        "\n",
        "    args.batch_size = 100\n",
        "    X = np.arange(40)\n",
        "    y = np.array(labels_bin[sub][:, type_emotion])\n",
        "    \n",
        "\n",
        "    skf = StratifiedKFold(n_splits=k, random_state=None, shuffle=True)\n",
        "    balanced_split = skf.split(X, y)\n",
        "    acc_sub = []\n",
        "    f1_sub = []\n",
        "   \n",
        "    for fold,  (ind_train, ind_test) in  enumerate(balanced_split):\n",
        "        inds_train = []\n",
        "        inds_test = []\n",
        "        inds_train.append(ind_train)\n",
        "        inds_test.append(ind_test)\n",
        "        print(f\"fold = {fold}\")\n",
        "        #print(inds_train, inds_test)\n",
        "        #print(sum(labels_bin[sub][inds_train, type_emotion]))\n",
        "        #print(sum(labels_bin[sub][inds_test, type_emotion]))\n",
        "        args.batch_size = 100\n",
        "        #transforms_random = RandomAugmentation([add_noise, reset_part_in_freq, reset_part_in_time, None], 0.2)\n",
        "        #transforms = [RandomAugmentation([add_noise(), reset_part_in_freq(0.2), reset_part_in_time(0.2), None], 0.2), to_head_matrix(),ToTensor()]   \n",
        "        transforms = [to_head_matrix(),ToTensor()] \n",
        "        #transforms = [RandomAugmentation([add_noise(), None], 0.5), to_head_matrix(),ToTensor()]   \n",
        "        #transforms = [RandomAugmentation([reset_part_in_time(0.4), None], 0.1), to_head_matrix(),ToTensor()]   \n",
        "\n",
        "        train_dataset = EmotionDataset_balanced(inds_train[0:1], data[sub:sub+1], labels_bin[sub:sub+1])\n",
        "       \n",
        "\n",
        "        train_dataloader = DataLoader(train_dataset, batch_size=args.batch_size, num_workers=1,\n",
        "                                        pin_memory=True, shuffle=True, drop_last=True)\n",
        "\n",
        "        val_dataset = EmotionDataset_balanced(inds_test[0:1], data[sub:sub+1], labels_bin[sub:sub+1])\n",
        "        val_dataloader = DataLoader(val_dataset, batch_size=args.batch_size, num_workers=1,\n",
        "                                      pin_memory=True, shuffle=False, drop_last=False)\n",
        "        device  = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        model = get_model(batch_norm = True, reg_extractor = True, ass_extractor = True)\n",
        "        model.apply(initialize_weights)\n",
        "        criterion = nn.CrossEntropyLoss(reduction = 'mean')#torch.nn.MSELoss()\n",
        "        #optimizer = optim.SGD(model.parameters(), lr=3e-5, momentum = 0.9)#, weight_decay=args.weight_decay)\n",
        "        optimizer = optim.Adam(model.parameters(), lr=3e-4)#, momentum = 0.9)#, weight_decay=args.weight_decay)\n",
        "        train_loop(f'batchnorm_Yes_batch_100_Adam_lr=3e4_{sub}_{fold}_subject_proper', 0, 20)\n",
        "        description = f'batchnorm_Yes_batch_100_Adam_lr=3e4_{sub}_{fold}_subject_proper'\n",
        "        model_state  = torch.load(os.path.join(args.output_dir, f\"val_{description}.tgz\"))\n",
        "        #   #model = Seq2Seq(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device)\n",
        "        model.load_state_dict(model_state['model_state_dict'])\n",
        "        acc, f1 = calculate_predictions(model, val_dataloader, type_emotion, True)\n",
        "        print(f\"f1 = {f1} acc = {acc}\")\n",
        "        acc_sub.append(acc)\n",
        "        f1_sub.append(f1)\n",
        "        print(acc, f1)\n",
        "    acc_all.append(acc_sub)     \n",
        "    f1_all.append(f1_sub)     \n",
        "    pd.DataFrame(f1_all).to_csv(\"f1_data_val_racnn_noleak.csv\")\n",
        "    pd.DataFrame(acc_all).to_csv(\"acc_data_val_racnn_noleak.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RK9UbNDRRh1"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "f1_data_val_noleak = pd.read_csv(\"f1_data_val_noleak.csv\")\n",
        "acc_data_val_noleak = pd.read_csv(\"acc_data_val_noleak.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YG1b5f4rSWZV"
      },
      "source": [
        "print(f1_data_val_noleak)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RiOJLWCSArq"
      },
      "source": [
        "f1 = f1_data_val_noleak[['0', '1', '2', '3' ,'4']].mean(axis = 1)\n",
        "acc = acc_data_val_noleak[['0', '1', '2', '3' ,'4']].mean(axis = 1)\n",
        "print(f1.mean())\n",
        "print(acc.mean())\n",
        "print(f1.std())\n",
        "print(acc.std())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOhoh5RDxFvY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3681BMfxGCt"
      },
      "source": [
        "Arrousal Correct Per sub model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H671UOJ5xGCv"
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold \n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "#def get_mertics_per_subject():\n",
        "k  = 5\n",
        "type_emotion = 1\n",
        "#acc_all = []\n",
        "#f1_all = []\n",
        "\n",
        "for sub in range(1, 32):\n",
        "    print(f\"*******{sub}*********\")\n",
        "\n",
        "    args.batch_size = 100\n",
        "    X = np.arange(40)\n",
        "    y = np.array(labels_bin[sub][:, type_emotion])\n",
        "    \n",
        "\n",
        "    skf = StratifiedKFold(n_splits=k, random_state=None, shuffle=True)\n",
        "    balanced_split = skf.split(X, y)\n",
        "    acc_sub = []\n",
        "    f1_sub = []\n",
        "   \n",
        "    for fold,  (ind_train, ind_test) in  enumerate(balanced_split):\n",
        "        inds_train = []\n",
        "        inds_test = []\n",
        "        inds_train.append(ind_train)\n",
        "        inds_test.append(ind_test)\n",
        "        print(f\"fold = {fold}\")\n",
        "        #print(inds_train, inds_test)\n",
        "        #print(sum(labels_bin[sub][inds_train, type_emotion]))\n",
        "        #print(sum(labels_bin[sub][inds_test, type_emotion]))\n",
        "        args.batch_size = 100\n",
        "        #transforms_random = RandomAugmentation([add_noise, reset_part_in_freq, reset_part_in_time, None], 0.2)\n",
        "        #transforms = [RandomAugmentation([add_noise(), reset_part_in_freq(0.2), reset_part_in_time(0.2), None], 0.2), to_head_matrix(),ToTensor()]   \n",
        "        transforms = [to_head_matrix(),ToTensor()] \n",
        "        #transforms = [RandomAugmentation([add_noise(), None], 0.5), to_head_matrix(),ToTensor()]   \n",
        "        #transforms = [RandomAugmentation([reset_part_in_time(0.4), None], 0.1), to_head_matrix(),ToTensor()]   \n",
        "\n",
        "        train_dataset = EmotionDataset_balanced(inds_train[0:1], data[sub:sub+1], labels_bin[sub:sub+1])\n",
        "       \n",
        "\n",
        "        train_dataloader = DataLoader(train_dataset, batch_size=args.batch_size, num_workers=1,\n",
        "                                        pin_memory=True, shuffle=True, drop_last=True)\n",
        "\n",
        "        val_dataset = EmotionDataset_balanced(inds_test[0:1], data[sub:sub+1], labels_bin[sub:sub+1])\n",
        "        val_dataloader = DataLoader(val_dataset, batch_size=args.batch_size, num_workers=1,\n",
        "                                      pin_memory=True, shuffle=False, drop_last=False)\n",
        "        device  = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        model = get_model(batch_norm = True, reg_extractor = True, ass_extractor = True)\n",
        "        model.apply(initialize_weights)\n",
        "        criterion = nn.CrossEntropyLoss(reduction = 'mean')#torch.nn.MSELoss()\n",
        "        #optimizer = optim.SGD(model.parameters(), lr=3e-5, momentum = 0.9)#, weight_decay=args.weight_decay)\n",
        "        optimizer = optim.Adam(model.parameters(), lr=3e-4)#, momentum = 0.9)#, weight_decay=args.weight_decay)\n",
        "        train_loop(f'batchnorm_Yes_batch_100_Adam_lr=3e4_{sub}_{fold}_subject_proper', 0, 20)\n",
        "        description = f'batchnorm_Yes_batch_100_Adam_lr=3e4_{sub}_{fold}_subject_proper'\n",
        "        model_state  = torch.load(os.path.join(args.output_dir, f\"val_{description}.tgz\"))\n",
        "        #   #model = Seq2Seq(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device)\n",
        "        model.load_state_dict(model_state['model_state_dict'])\n",
        "        acc, f1 = calculate_predictions(model, val_dataloader, type_emotion)\n",
        "        print(f\"f1 = {f1} acc = {acc}\")\n",
        "        acc_sub.append(acc)\n",
        "        f1_sub.append(f1)\n",
        "        print(acc, f1)\n",
        "    acc_all.append(acc_sub)     \n",
        "    f1_all.append(f1_sub)     \n",
        "    pd.DataFrame(f1_all).to_csv(\"f1_data_val_noleak.csv\")\n",
        "    pd.DataFrame(acc_all).to_csv(\"acc_data_val_noleak.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxjykXGHkW0C"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4stg9ly-lk0p"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "f1_data_arousal_noleak = pd.read_csv(\"f1_data_racnn_arousal_noleak.csv\")\n",
        "acc_data_arousal_noleak = pd.read_csv(\"acc_data_racnn_arousal_noleak.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UIdpPCxnPJi"
      },
      "source": [
        "print(f1_data_arousal_noleak)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxsIDlDKnYce"
      },
      "source": [
        "\n",
        "print(acc_data_arousal_noleak)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6RniMXBlk0z"
      },
      "source": [
        "f1 = f1_data_arousal_noleak[['0', '1', '2', '3' ,'4']].mean(axis = 1)\n",
        "acc = acc_data_arousal_noleak[['0', '1', '2', '3' ,'4']].mean(axis = 1)\n",
        "print(f1.mean())\n",
        "print(acc.mean())\n",
        "print(f1.std())\n",
        "print(acc.std())"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}